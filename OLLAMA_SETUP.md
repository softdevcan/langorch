# Ollama Setup Guide - LangOrch

## Overview

Ollama artÄ±k LangOrch'un docker-compose yapÄ±landÄ±rmasÄ±na dahil edilmiÅŸtir. Bu sayede lokal embedding modelleri kullanabilirsiniz.

## ğŸš€ Quick Start

### 1. Ollama'yÄ± Docker ile BaÅŸlatÄ±n

```bash
# TÃ¼m servisleri baÅŸlat (Ollama dahil)
docker-compose up -d

# Sadece Ollama'yÄ± baÅŸlat
docker-compose up -d ollama

# Ollama loglarÄ±nÄ± kontrol et
docker-compose logs -f ollama
```

### 2. Embedding Modellerini Ä°ndirin

#### Option A: Manuel Ä°ndirme (Recommended)

```bash
# Container iÃ§ine gir
docker exec -it langorch_ollama bash

# Model indir
ollama pull nomic-embed-text        # 768 dims, recommended
ollama pull mxbai-embed-large       # 1024 dims, high quality
ollama pull all-minilm              # 384 dims, fast

# Modelleri listele
ollama list

# Container'dan Ã§Ä±k
exit
```

#### Option B: Initialization Script

```bash
# Script'i container iÃ§inde Ã§alÄ±ÅŸtÄ±r
docker exec -it langorch_ollama bash /app/init-models.sh
```

Veya Windows PowerShell'de:

```powershell
# Script'i container'a kopyala ve Ã§alÄ±ÅŸtÄ±r
docker cp infrastructure/docker/ollama/init-models.sh langorch_ollama:/tmp/init-models.sh
docker exec -it langorch_ollama bash /tmp/init-models.sh
```

### 3. LangOrch Settings'de YapÄ±landÄ±rÄ±n

1. Frontend'i aÃ§Ä±n: http://localhost:3000
2. Login: `admin@test.com` / `admin123`
3. Settings â†’ Embedding Provider'a gidin
4. **Provider**: Ollama seÃ§in
5. **Model**: `nomic-embed-text` seÃ§in
6. **Ollama URL**: `http://ollama:11434` (Docker network iÃ§inde)
   - **Not**: EÄŸer backend Docker dÄ±ÅŸÄ±ndaysa: `http://localhost:11434`
7. "Test Connection" â†’ BaÅŸarÄ±lÄ± olmalÄ± âœ…
8. "Save Settings" â†’ Kaydedin

## ğŸ“Š Model Comparison

| Model | Dimensions | Size | Speed | Use Case |
|-------|-----------|------|-------|----------|
| **nomic-embed-text** | 768 | ~274MB | Medium | General purpose (recommended) |
| **mxbai-embed-large** | 1024 | ~669MB | Slower | High quality embeddings |
| **all-minilm** | 384 | ~46MB | Fast | Fast processing, less accuracy |

## ğŸ” Testing

### Test Ollama Availability

```bash
# From host
curl http://localhost:11434/api/version

# Expected response:
# {"version":"0.x.x"}
```

### Test Model Availability

```bash
# List models
docker exec langorch_ollama ollama list

# Test embedding generation
curl http://localhost:11434/api/embeddings \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Hello, this is a test"
  }'
```

## ğŸ› Troubleshooting

### Issue: Ollama container not starting

```bash
# Check logs
docker-compose logs ollama

# Restart container
docker-compose restart ollama
```

### Issue: Models not downloading

```bash
# Check disk space
docker exec langorch_ollama df -h

# Check network
docker exec langorch_ollama ping -c 3 ollama.ai

# Try manual pull
docker exec -it langorch_ollama ollama pull nomic-embed-text
```

### Issue: Connection refused from backend

**If backend is in Docker:**
- Use `http://ollama:11434` (service name)

**If backend is on host (outside Docker):**
- Use `http://localhost:11434`

**If using WSL:**
- Use `http://host.docker.internal:11434`

### Issue: Model too slow

Try a smaller model:
```bash
docker exec langorch_ollama ollama pull all-minilm
```

Then update Settings to use `all-minilm` model.

## ğŸ“ Configuration

### Environment Variables

Add to `.env` file (optional):

```env
# Ollama Configuration
OLLAMA_HOST=0.0.0.0
OLLAMA_PORT=11434
OLLAMA_MODELS=nomic-embed-text,mxbai-embed-large,all-minilm
```

### Custom Models

Ä°stediÄŸiniz baÅŸka Ollama modelini de kullanabilirsiniz:

```bash
# Search for embedding models
docker exec langorch_ollama ollama search embed

# Pull custom model
docker exec langorch_ollama ollama pull <model-name>
```

## ğŸ”„ Switching Between Providers

### From OpenAI to Ollama

1. Go to Settings â†’ Embedding Provider
2. Change provider to **Ollama**
3. Select model: **nomic-embed-text**
4. URL: `http://ollama:11434` (or `http://localhost:11434`)
5. Test Connection
6. Save Settings
7. **Reprocess existing documents** (optional):
   - Go to Documents page
   - Click "Reprocess" on each document
   - Documents will be re-embedded with Ollama

### From Ollama to OpenAI

1. Go to Settings â†’ Embedding Provider
2. Change provider to **OpenAI**
3. Select model: **text-embedding-3-small**
4. Enter API Key
5. Test Connection
6. Save Settings

## ğŸ’¡ Tips

1. **Performance**: `nomic-embed-text` offers best balance of speed/quality
2. **Cost**: Ollama is 100% free, runs locally
3. **Privacy**: All data stays on your machine
4. **Offline**: Works without internet (after models are downloaded)
5. **GPU**: If you have NVIDIA GPU, Ollama will automatically use it

## ğŸ“š Resources

- [Ollama Official Docs](https://ollama.ai/docs)
- [Ollama Embedding Models](https://ollama.ai/search?c=embedding)
- [LangOrch Documentation](./docs/README.md)

## ğŸ¯ Next Steps

After setup:
1. Upload a test document
2. Verify it's processed with Ollama
3. Try semantic search
4. Compare OpenAI vs Ollama results

---

**Created**: December 27, 2024
**Version**: 0.2.5
**Maintained By**: LangOrch Team
